{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d745511-67b0-4a04-a65b-94222d571321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f19899-db68-4ef8-ad2a-f6310fd33d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = 'edge_iiot_preprocessed.csv'  # Replace with your actual path\n",
    "df = pd.read_csv(preprocessed_dataset, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27f7ba8-d5ee-4647-a286-f0ceb08b9bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_difference</th>\n",
       "      <th>arp.opcode</th>\n",
       "      <th>arp.hw.size</th>\n",
       "      <th>icmp.checksum</th>\n",
       "      <th>icmp.seq_le</th>\n",
       "      <th>http.content_length</th>\n",
       "      <th>http.request.method</th>\n",
       "      <th>http.referer</th>\n",
       "      <th>http.request.version</th>\n",
       "      <th>http.response</th>\n",
       "      <th>...</th>\n",
       "      <th>mqtt.hdrflags</th>\n",
       "      <th>mqtt.len</th>\n",
       "      <th>mqtt.msgtype</th>\n",
       "      <th>mqtt.proto_len</th>\n",
       "      <th>mqtt.protoname</th>\n",
       "      <th>mqtt.topic</th>\n",
       "      <th>mqtt.topic_len</th>\n",
       "      <th>mqtt.ver</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>Attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000309</td>\n",
       "      <td>-0.047835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.219227</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.235181</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300891</td>\n",
       "      <td>-0.281368</td>\n",
       "      <td>-0.300891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.460693</td>\n",
       "      <td>-0.460680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.047835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.219227</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.235181</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044291</td>\n",
       "      <td>1.180232</td>\n",
       "      <td>0.044291</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.395124</td>\n",
       "      <td>-0.460680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.047835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.219227</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.235181</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300891</td>\n",
       "      <td>-0.281368</td>\n",
       "      <td>-0.300891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.460693</td>\n",
       "      <td>-0.460680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.047835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.219227</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.235181</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389473</td>\n",
       "      <td>-0.037768</td>\n",
       "      <td>0.389473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.460693</td>\n",
       "      <td>-0.460680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.047835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.219227</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.235181</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734654</td>\n",
       "      <td>4.468833</td>\n",
       "      <td>0.734654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.460693</td>\n",
       "      <td>3.395354</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_difference  arp.opcode  arp.hw.size  icmp.checksum  icmp.seq_le  \\\n",
       "0        -0.000309   -0.047835          0.0      -0.219227    -0.220062   \n",
       "1         0.000818   -0.047835          0.0      -0.219227    -0.220062   \n",
       "2        -0.000308   -0.047835          0.0      -0.219227    -0.220062   \n",
       "3        -0.000304   -0.047835          0.0      -0.219227    -0.220062   \n",
       "4        -0.000260   -0.047835          0.0      -0.219227    -0.220062   \n",
       "\n",
       "   http.content_length  http.request.method  http.referer  \\\n",
       "0            -0.040216             0.131863      0.235181   \n",
       "1            -0.040216             0.131863      0.235181   \n",
       "2            -0.040216             0.131863      0.235181   \n",
       "3            -0.040216             0.131863      0.235181   \n",
       "4            -0.040216             0.131863      0.235181   \n",
       "\n",
       "   http.request.version  http.response  ...  mqtt.hdrflags  mqtt.len  \\\n",
       "0             -0.005951              0  ...      -0.300891 -0.281368   \n",
       "1             -0.005951              0  ...       0.044291  1.180232   \n",
       "2             -0.005951              0  ...      -0.300891 -0.281368   \n",
       "3             -0.005951              0  ...       0.389473 -0.037768   \n",
       "4             -0.005951              0  ...       0.734654  4.468833   \n",
       "\n",
       "   mqtt.msgtype  mqtt.proto_len  mqtt.protoname  mqtt.topic  mqtt.topic_len  \\\n",
       "0     -0.300891             0.0       -0.460693   -0.460680             0.0   \n",
       "1      0.044291             4.0        3.395124   -0.460680             0.0   \n",
       "2     -0.300891             0.0       -0.460693   -0.460680             0.0   \n",
       "3      0.389473             0.0       -0.460693   -0.460680             0.0   \n",
       "4      0.734654             0.0       -0.460693    3.395354            24.0   \n",
       "\n",
       "   mqtt.ver  Attack_label  Attack_type  \n",
       "0       0.0             0       Normal  \n",
       "1       4.0             0       Normal  \n",
       "2       0.0             0       Normal  \n",
       "3       0.0             0       Normal  \n",
       "4       0.0             0       Normal  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a602ee7-ad0e-4959-8bf5-74f1e5e56868",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Attack_label', 'Attack_type'], axis=1)\n",
    "y = df['Attack_label']  # for binary classification (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24019661-c166-443d-aaf8-a86e364531cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This function gives the number of packets in the last 1 second (approx)\n",
    "def packets_last_10s(time_diff_series, threshold=1.0):\n",
    "    result = []\n",
    "    for i in range(len(time_diff_series)):\n",
    "        total_time = 0\n",
    "        count = 0\n",
    "        j = i\n",
    "        while j >= 0 and total_time <= threshold:\n",
    "            total_time += time_diff_series.iloc[j]\n",
    "            count += 1\n",
    "            j -= 1\n",
    "        result.append(count)\n",
    "    return result\n",
    "\n",
    "X['packet_count_10s'] = packets_last_10s(X['time_difference'], threshold=10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8197588-2f80-434f-b6ff-7c53bc13d3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_difference', 'arp.opcode', 'arp.hw.size', 'icmp.checksum',\n",
       "       'icmp.seq_le', 'http.content_length', 'http.request.method',\n",
       "       'http.referer', 'http.request.version', 'http.response', 'tcp.ack',\n",
       "       'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin',\n",
       "       'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack',\n",
       "       'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.seq', 'udp.stream',\n",
       "       'udp.time_delta', 'dns.qry.name', 'dns.qry.qu', 'dns.retransmission',\n",
       "       'dns.retransmit_request', 'dns.retransmit_request_in',\n",
       "       'mqtt.conack.flags', 'mqtt.conflag.cleansess', 'mqtt.conflags',\n",
       "       'mqtt.hdrflags', 'mqtt.len', 'mqtt.msgtype', 'mqtt.proto_len',\n",
       "       'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a17c1-8a4c-4854-86b0-587c3c157534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Drop non-numeric or high-cardinality categorical columns\n",
    "# columns_to_drop = ['mqtt.topic', 'dns.qry.name']  # you can adjust\n",
    "X_clean = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Step 2: Scale features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Step 3: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Step 5: Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a74a5f-68ef-4829-b932-b1c3bbd91987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear((num_features // 2) * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)  # 2 classes for Attack_label\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, num_features)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> (batch_size, 8, num_features//2)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee921bbd-b676-41a4-b1e9-dc10c2456644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_np and y_np are your input features and target labels as NumPy arrays\n",
    "\n",
    "# Step 1: Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Step 3: Modified fitness function with L2 regularization and validation loss\n",
    "def fitness(position):\n",
    "    weights = position[:X_train.shape[1]].reshape(-1, 1)  # Shape it for logistic regression (n_features, 1)\n",
    "    bias = position[-1]  # Last value is bias\n",
    "    \n",
    "    # Calculate logits and probabilities for training set\n",
    "    logits_train = X_train @ weights + bias\n",
    "    probs_train = sigmoid(logits_train)\n",
    "    \n",
    "    # Calculate the training loss (binary cross-entropy)\n",
    "    train_loss = -np.mean(y_train * np.log(probs_train + 1e-8) + (1 - y_train) * np.log(1 - probs_train + 1e-8))\n",
    "    \n",
    "    # L2 regularization (Ridge regularization)\n",
    "    lambda_reg = 0.01\n",
    "    train_loss += lambda_reg * np.sum(weights**2)\n",
    "\n",
    "    # Calculate logits and probabilities for validation set\n",
    "    logits_val = X_val @ weights + bias\n",
    "    probs_val = sigmoid(logits_val)\n",
    "    \n",
    "    # Calculate the validation loss (binary cross-entropy)\n",
    "    val_loss = -np.mean(y_val * np.log(probs_val + 1e-8) + (1 - y_val) * np.log(1 - probs_val + 1e-8))\n",
    "    \n",
    "    # Combine training and validation loss\n",
    "    total_loss = train_loss + val_loss\n",
    "    return total_loss\n",
    "\n",
    "# Step 4: Initialize PSO parameters\n",
    "n_particles = 30\n",
    "n_iterations = 10\n",
    "n_features = X_train.shape[1]\n",
    "global_best_position = np.random.randn(n_features + 1)  # Weights + bias\n",
    "global_best_loss = float('inf')\n",
    "particles_positions = np.random.randn(n_particles, n_features + 1)\n",
    "particles_velocities = np.random.randn(n_particles, n_features + 1)\n",
    "particles_best_positions = particles_positions.copy()\n",
    "particles_best_losses = np.array([fitness(p) for p in particles_positions])\n",
    "\n",
    "# Step 5: Run the PSO algorithm\n",
    "for iteration in range(n_iterations):\n",
    "    for i, position in enumerate(particles_positions):\n",
    "        # Evaluate fitness (loss) for each particle\n",
    "        current_loss = fitness(position)\n",
    "        \n",
    "        # Update personal best if current loss is better\n",
    "        if current_loss < particles_best_losses[i]:\n",
    "            particles_best_losses[i] = current_loss\n",
    "            particles_best_positions[i] = position\n",
    "            \n",
    "        # Update global best if the personal best is better\n",
    "        if current_loss < global_best_loss:\n",
    "            global_best_loss = current_loss\n",
    "            global_best_position = position\n",
    "\n",
    "    # Update the velocity and position for each particle\n",
    "    inertia = 0.5\n",
    "    c1, c2 = 2.0, 2.0\n",
    "    r1, r2 = np.random.rand(n_particles, n_features + 1), np.random.rand(n_particles, n_features + 1)\n",
    "    \n",
    "    particles_velocities = (inertia * particles_velocities + \n",
    "                            c1 * r1 * (particles_best_positions - particles_positions) + \n",
    "                            c2 * r2 * (global_best_position - particles_positions))\n",
    "    \n",
    "    particles_positions += particles_velocities\n",
    "    \n",
    "    # Print progress for each iteration\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations} - Best Loss: {global_best_loss:.4f}\")\n",
    "\n",
    "# Step 6: After optimization, test the best model\n",
    "weights = global_best_position[:n_features].reshape(-1, 1)\n",
    "bias = global_best_position[-1]\n",
    "\n",
    "# Predict on validation set\n",
    "logits_val = X_val @ weights + bias\n",
    "probs_val = sigmoid(logits_val)\n",
    "y_pred = (probs_val >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e5de76-1d51-411a-af13-12f175596396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best model (weights and bias) to a file using pickle\n",
    "model = {\n",
    "    'weights': global_best_position[:n_features].reshape(-1, 1),\n",
    "    'bias': global_best_position[-1]\n",
    "}\n",
    "\n",
    "# Save the model to a file\n",
    "with open('pso_optimized_logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ba3849e-0f96-4a2d-a858-d97031b6ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Batch 23810/23810  Train Accuracy: 100.00%  Test Accuracy: 100.00%\n",
      "Epoch 2/10 - Batch 23810/23810  Train Accuracy: 100.00%  Test Accuracy: 100.00%\n",
      "Epoch 3/10 - Batch 23810/23810  Train Accuracy: 100.00%  Test Accuracy: 100.00%\n",
      "Epoch 4/10 - Batch 21400/23810"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m outputs = model(batch_x)\n\u001b[32m     12\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m optimizer.step()\n\u001b[32m     16\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(\n",
    "                f\"\\rEpoch {epoch+1}/{epochs} - Batch {batch_idx+1}/{len(train_loader)}\", \n",
    "                end=\"\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"  Train Accuracy: {train_acc:.2f}%\", end=\"\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"  Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adi-kernel",
   "language": "python",
   "name": "adi-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
