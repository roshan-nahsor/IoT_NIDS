{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef370034-963b-4351-8b4f-4ffc3f3ef72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e951a2-442a-4cf8-b91f-c48f553910d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame.time</th>\n",
       "      <th>ip.src_host</th>\n",
       "      <th>ip.dst_host</th>\n",
       "      <th>arp.dst.proto_ipv4</th>\n",
       "      <th>arp.opcode</th>\n",
       "      <th>arp.hw.size</th>\n",
       "      <th>arp.src.proto_ipv4</th>\n",
       "      <th>icmp.checksum</th>\n",
       "      <th>icmp.seq_le</th>\n",
       "      <th>icmp.transmit_timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>mqtt.proto_len</th>\n",
       "      <th>mqtt.protoname</th>\n",
       "      <th>mqtt.topic</th>\n",
       "      <th>mqtt.topic_len</th>\n",
       "      <th>mqtt.ver</th>\n",
       "      <th>mbtcp.len</th>\n",
       "      <th>mbtcp.trans_id</th>\n",
       "      <th>mbtcp.unit_id</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>Attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021 11:44:10.081753000</td>\n",
       "      <td>192.168.0.128</td>\n",
       "      <td>192.168.0.101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021 11:44:10.162218000</td>\n",
       "      <td>192.168.0.101</td>\n",
       "      <td>192.168.0.128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>MQTT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021 11:44:10.162271000</td>\n",
       "      <td>192.168.0.128</td>\n",
       "      <td>192.168.0.101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021 11:44:10.162641000</td>\n",
       "      <td>192.168.0.128</td>\n",
       "      <td>192.168.0.101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021 11:44:10.166132000</td>\n",
       "      <td>192.168.0.101</td>\n",
       "      <td>192.168.0.128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Temperature_and_Humidity</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  frame.time    ip.src_host    ip.dst_host arp.dst.proto_ipv4  \\\n",
       "0   2021 11:44:10.081753000   192.168.0.128  192.168.0.101                  0   \n",
       "1   2021 11:44:10.162218000   192.168.0.101  192.168.0.128                  0   \n",
       "2   2021 11:44:10.162271000   192.168.0.128  192.168.0.101                  0   \n",
       "3   2021 11:44:10.162641000   192.168.0.128  192.168.0.101                  0   \n",
       "4   2021 11:44:10.166132000   192.168.0.101  192.168.0.128                  0   \n",
       "\n",
       "   arp.opcode  arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
       "0         0.0          0.0                  0            0.0          0.0   \n",
       "1         0.0          0.0                  0            0.0          0.0   \n",
       "2         0.0          0.0                  0            0.0          0.0   \n",
       "3         0.0          0.0                  0            0.0          0.0   \n",
       "4         0.0          0.0                  0            0.0          0.0   \n",
       "\n",
       "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  \\\n",
       "0                      0.0  ...             0.0              0   \n",
       "1                      0.0  ...             4.0           MQTT   \n",
       "2                      0.0  ...             0.0              0   \n",
       "3                      0.0  ...             0.0              0   \n",
       "4                      0.0  ...             0.0              0   \n",
       "\n",
       "                 mqtt.topic mqtt.topic_len mqtt.ver mbtcp.len mbtcp.trans_id  \\\n",
       "0                         0            0.0      0.0       0.0            0.0   \n",
       "1                         0            0.0      4.0       0.0            0.0   \n",
       "2                         0            0.0      0.0       0.0            0.0   \n",
       "3                         0            0.0      0.0       0.0            0.0   \n",
       "4  Temperature_and_Humidity           24.0      0.0       0.0            0.0   \n",
       "\n",
       "  mbtcp.unit_id  Attack_label  Attack_type  \n",
       "0           0.0             0       Normal  \n",
       "1           0.0             0       Normal  \n",
       "2           0.0             0       Normal  \n",
       "3           0.0             0       Normal  \n",
       "4           0.0             0       Normal  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'F:/Documents/CRCE/Project/NIDS/dataset/Edge-IIoT/Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv'  # Replace with your actual path\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9ed42d-e514-453c-8290-7e7554564ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack_type\n",
      "Normal                   1615643\n",
      "DDoS_UDP                  121568\n",
      "DDoS_ICMP                 116436\n",
      "SQL_injection              51203\n",
      "Password                   50153\n",
      "Vulnerability_scanner      50110\n",
      "DDoS_TCP                   50062\n",
      "DDoS_HTTP                  49911\n",
      "Uploading                  37634\n",
      "Backdoor                   24862\n",
      "Port_Scanning              22564\n",
      "XSS                        15915\n",
      "Ransomware                 10925\n",
      "MITM                        1214\n",
      "Fingerprinting              1001\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.head(5)\n",
    "print(df['Attack_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d1742a-f02b-416d-b28d-7520cfb4b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack_type\n",
      "Normal                   1363998\n",
      "DDoS_UDP                  121567\n",
      "DDoS_ICMP                  67939\n",
      "SQL_injection              50826\n",
      "DDoS_TCP                   50062\n",
      "Vulnerability_scanner      50026\n",
      "Password                   49933\n",
      "DDoS_HTTP                  48544\n",
      "Uploading                  36807\n",
      "Backdoor                   24026\n",
      "Port_Scanning              19977\n",
      "XSS                        15066\n",
      "Ransomware                  9689\n",
      "Fingerprinting               853\n",
      "MITM                         358\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "drop_columns = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n",
    "         \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
    "         \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
    "         \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n",
    "\n",
    "df.drop(drop_columns, axis=1, inplace=True)\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
    "df = shuffle(df)\n",
    "df.isna().sum()\n",
    "\n",
    "print(df['Attack_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9376df30-7d78-48bf-a452-3060ac48a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(df,'http.request.method')\n",
    "encode_text_dummy(df,'http.referer')\n",
    "encode_text_dummy(df,\"http.request.version\")\n",
    "encode_text_dummy(df,\"dns.qry.name.len\")\n",
    "encode_text_dummy(df,\"mqtt.conack.flags\")\n",
    "encode_text_dummy(df,\"mqtt.protoname\")\n",
    "encode_text_dummy(df,\"mqtt.topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac92bffb-ec12-43f0-8e04-4fbd58d2e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Step 3: Encode labels\n",
    "label_encoder = {label: idx for idx, label in enumerate(df[\"Attack_type\"].unique())}\n",
    "df[\"Attack_type\"] = df[\"Attack_type\"].map(label_encoder)\n",
    "\n",
    "# Step 4: Split features and target\n",
    "X = df.drop([\"Attack_label\", \"Attack_type\"], axis=1).values\n",
    "y = df[\"Attack_type\"].values\n",
    "\n",
    "# Step 5: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Reshape for CNN input: (batch_size, channels, features)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635e703c-8d76-4862-b992-442be53a98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "num_class = 15\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 16, kernel_size=3, padding=1)        \n",
    "        self.fc1 = nn.Linear(95 * 16, 30)\n",
    "        self.out_layer = nn.Linear(30, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        # conv layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        # conv layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        # conv layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=1)\n",
    "        # Flatten layer \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        # fc layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # Output Layer \n",
    "        x = self.out_layer(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "343317a1-d54c-495a-91a5-564f2e2e6f92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mmodel\u001b[49m.train()\n\u001b[32m      7\u001b[39m     total_loss = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m     num_batches = \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Overwrite the batch number on the same line\n",
    "        print(f\"\\rEpoch {epoch+1} | Batch {batch_idx+1}/{num_batches}\", end='', flush=True)\n",
    "    print(f\"\\nTotal Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nTotal Loss: {total_loss:.4f} \")\n",
    "    # | Validation Accuracy: {acc*100:.2f}%\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b85e3b-a73b-4718-8126-eeb9ebc63004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 23871/23871\n",
      "Total Loss: 2742.7554\n",
      "\n",
      "Total Loss: 2742.7554 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosha\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:552: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\rosha\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# | Validation Accuracy: {acc*100:.2f}%\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2724\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2722\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2723\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2724\u001b[39m     name_width = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2725\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   2726\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
      "\u001b[31mValueError\u001b[39m: max() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Step 1: Clean & shuffle the DataFrame\n",
    "drop_columns = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\", \"arp.dst.proto_ipv4\",\n",
    "                \"http.file_data\", \"http.request.full_uri\", \"icmp.transmit_timestamp\", \"http.request.uri.query\",\n",
    "                \"tcp.options\", \"tcp.payload\", \"tcp.srcport\", \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n",
    "\n",
    "df.drop(drop_columns, axis=1, inplace=True)\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = shuffle(df)\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name], prefix=name)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "for col in [\"http.request.method\", \"http.referer\", \"http.request.version\",\n",
    "            \"dns.qry.name.len\", \"mqtt.conack.flags\", \"mqtt.protoname\", \"mqtt.topic\"]:\n",
    "    df = encode_text_dummy(df, col)\n",
    "\n",
    "# Step 3: Encode labels\n",
    "label_encoder = {label: idx for idx, label in enumerate(df[\"Attack_type\"].unique())}\n",
    "df[\"Attack_type\"] = df[\"Attack_type\"].map(label_encoder)\n",
    "\n",
    "# Step 4: Split features and target\n",
    "X = df.drop([\"Attack_label\", \"Attack_type\"], axis=1).values\n",
    "y = df[\"Attack_type\"].values\n",
    "\n",
    "# Step 5: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Reshape for CNN input: (batch_size, channels, features)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Step 8: Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(95 * 16, 30)  # 95 is input feature count, adjust if needed\n",
    "        self.out_layer = nn.Linear(30, 15)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.log_softmax(self.out_layer(x), dim=1)\n",
    "\n",
    "# Step 9: Train the CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Overwrite the batch number on the same line\n",
    "        print(f\"\\rEpoch {epoch+1} | Batch {batch_idx+1}/{num_batches}\", end='', flush=True)\n",
    "    print(f\"\\nTotal Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_X, batch_y in val_loader:\n",
    "    #         batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "    #         output = model(batch_X)\n",
    "    #         preds = torch.argmax(output, dim=1)\n",
    "    #         all_preds.extend(preds.cpu().numpy())\n",
    "    #         all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nTotal Loss: {total_loss:.4f} \")\n",
    "    # | Validation Accuracy: {acc*100:.2f}%\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da44f0cc-dcd1-4ab7-a5f7-ff37fcd9c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is already preprocessed and encoded\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop([\"Attack_label\", \"Attack_type\"], axis=1).values\n",
    "y = df[\"Attack_label\"].values  # or a different target if you're using multiclass\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # shape (batch, 1, features)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a18b7e80-6556-46c9-893c-699518607184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 23871/23871\n",
      "Total Loss: 45.9101 | Validation Accuracy: 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000    272800\n",
      "           1     1.0000    1.0000    1.0000    109135\n",
      "\n",
      "    accuracy                         1.0000    381935\n",
      "   macro avg     1.0000    1.0000    1.0000    381935\n",
      "weighted avg     1.0000    1.0000    1.0000    381935\n",
      "\n",
      "Epoch 2 | Batch 2813/23871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\u001b[32m     10\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = criterion(output, batch_y)\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     71\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x))\n\u001b[32m     72\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     75\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.fc1(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Jupyter\\NIDS\\new_venv\\adi-venv\\Lib\\site-packages\\torch\\nn\\functional.py:740\u001b[39m, in \u001b[36m_max_pool1d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     stride = torch.jit.annotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print live batch progress like: Epoch 1 | Batch 12/87\n",
    "        print(f\"\\rEpoch {epoch+1} | Batch {batch_idx+1}/{num_batches}\", end='', flush=True)\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            output = model(batch_X)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nTotal Loss: {total_loss:.4f} | Validation Accuracy: {acc*100:.2f}%\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adi-kernel",
   "language": "python",
   "name": "adi-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
